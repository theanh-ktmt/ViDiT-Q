model:
    model_id: "pixart"
    model_type: 'pixart'  # ['sd','sdxl']
conditional: True
smooth_quant_list: ""
calib_data:
    path: "logs/pixart/pixart_alpha/calib_data"  # the path of the calib dataset
    n_steps: 19
    batch_size: 4  # used for quant param initialization
    n_samples: 32
    iters_w: 20000
    iters_a: 5000
quant:
    weight:
        optimization:
            # assume that weight/act quantize param opt together
            # joint_weight_act_opt: False
            # use_grad: False
            # iters: 1000
            # loss:
            #     lambda_coeff: 0.5
            #     warmup: 0.0
            # params:
            #     delta: 
            #         lr: 1.e-4
            #     alpha: 
            #         lr: 1.e-2
        quantizer:
            n_bits: 8
            per_group: "channel"
            scale_method: 'min_max'
            # round_mode: 'learned_hard_sigmoid'
            round_mode: 'nearest'
    activation:
        optimization:
            # joint_weight_act_opt: False
            # use_grad: False
            # iters: 500
            # loss:
            #     lambda_coeff: 1.0
            #     warmup: 0.0
            # params:
            #     delta:
            #         lr: 1.e-4
            #      #alpha: 
            #          #lr: 1.e-2
        quantizer:
            n_bits: 8
            per_group: False
            scale_method: 'min_max'
            dynamic: False
            round_mode: 'nearest_ste'
            running_stat: False
            # timestep_wise: True
            softmax: 
                # n_bits: 16
                # channel_wise: False
                # scale_method: 'min_max'
                # round_mode: 'nearest'
                # always_zero: True
            smooth_quant: # les: it is only for convenience to put smooth quant configs under act quantizer
                enable: False
                channel_wise_scale_type: momentum_act_max
                momentum: 0.95
                alpha: 0.3

    grad_checkpoint: False
resume_with_w_quantized:
    path: None
